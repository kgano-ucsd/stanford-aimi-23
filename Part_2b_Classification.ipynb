{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## AIMI High School Internship 2023\n",
        "### Notebook 2: Training a Vision Model to Predict ET Distances\n",
        "\n",
        "**The Problem**: Given a chest X-ray, our goal in this project is to predict the distance from an endotracheal tube to the carina. This is an important clinical task - endotracheal tubes that are positioned too far (>5cm) above the carina will not work effectively.\n",
        "\n",
        "**Your Second Task**: You should now have a training dataset consisting of (a) chest X-rays and (b) annotations indicating the distance of the endotracheal tube from the carina. Now, your goal is to train a computer vision model to predict endotracheal tube distance from the image. You have **two options** for this task, and you may attempt one or both of these:\n",
        "- *Distance Categorization* : Train a model to determine whether the position of a tube is abnormal (>5.0 cm) or normal (â‰¤ 5.0 cm).\n",
        "- *Distance Prediction*: Train a model that predicts the distance of the endotracheal tube from the carina in centimeters.\n",
        "\n",
        "In this notebook, we provide some simple starter code to get you started on training a computer vision model. You are not required to use this template - feel free to modify as you see fit.\n",
        "\n",
        "**Submitting Your Model**: We have created a leaderboard where you can submit your model and view results on the held-out test set. We provide instructions below for submitting your model to the leaderboard. **Please follow these directions carefully**.\n",
        "\n",
        "We will evaluate your results on the held-out test set with the following evaluation metrics:\n",
        "- *Distance Categorization* : We will measure AUROC, which is a metric commonly used in healthcare tasks. See this blog for a good explanation of AUROC: https://glassboxmedicine.com/2019/02/23/measuring-performance-auc-auroc/\n",
        "- *Distance Prediction*: We will measure the mean average error (also known as L1 distance) between the predicted distances and the true distances.\n"
      ],
      "metadata": {
        "id": "uMfGLGNLnXk7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data\n",
        "Before you begin, make sure to go to `Runtime` > `Change Runtime Type` and select a T4 GPU. Then, upload `data.zip`. It should take about 10 minutes for these files to be uploaded. Then, run the following cells to unzip the dataset (which should take < 10 seconds)"
      ],
      "metadata": {
        "id": "RJ1rTMrgpoil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "b5J5Ks2TGWJW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "637e371a-5514-40e6-cf03-80846dd086f0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq /content/drive/MyDrive/kams_colab/mimic-train.zip"
      ],
      "metadata": {
        "id": "ySb9AsmBp-Gz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hsAHG4p4WR6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq /content/drive/MyDrive/kams_colab/mimic-test.zip"
      ],
      "metadata": {
        "id": "Bqeesviqp_hN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries\n",
        "We are leveraging the PyTorch framework to train our models. For more information and tutorials on PyTorch, see this link: https://pytorch.org/tutorials/beginner/basics/intro.html"
      ],
      "metadata": {
        "id": "q5etX4eYtu_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some libraries that you may find useful are included here.\n",
        "# To import a library that isn't provided with Colab, use the following command: !pip install torchmetrics\n",
        "import torch\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, random_split\n"
      ],
      "metadata": {
        "id": "BzhTFDi7tuPK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Dataloaders\n",
        "We will implement a custom Dataset class to load in data. A custom Dataset class must have three methods: `__init__`, which sets up any class variables, `__len__`, which defines the total number of images, and `__getitem__`, which returns a single image and its paired label."
      ],
      "metadata": {
        "id": "QY7yvIM0yl4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class ChestXRayDataset(Dataset):\n",
        "    def __init__(self, img_paths, labels, distances):\n",
        "        super(ChestXRayDataset, self).__init__()\n",
        "        self.img_paths = img_paths\n",
        "        self.labels = labels\n",
        "        self.distances = distances\n",
        "        # Fill in __init__() here\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        # Fill in __len__() here\n",
        "        return self.labels.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        out_dict = {\"idx\": torch.tensor(idx),}\n",
        "\n",
        "        # Fill in __getitem__() here\n",
        "        im = Image.open(f\"/content/{self.img_paths[idx]}\")\n",
        "\n",
        "        w, h = im.size\n",
        "        ima = Image.new('RGB', (w,h))\n",
        "        data = zip(im.getdata(), im.getdata(), im.getdata())\n",
        "        ima.putdata(list(data))\n",
        "        convert_tensor = transforms.Compose([\n",
        "            transforms.Resize(size=224),\n",
        "            transforms.RandomEqualize(p=0.5),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "        img_as_tensor = convert_tensor(ima)\n",
        "        img_as_tensor.requires_grad_ = True\n",
        "        out_dict[\"img\"] = img_as_tensor\n",
        "        out_dict[\"labels\"] = self.labels[idx]\n",
        "        out_dict[\"distance\"] = self.distances[idx]\n",
        "\n",
        "        return out_dict\n",
        "print(device)"
      ],
      "metadata": {
        "id": "FwH5586UqAnY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e41f4f1a-d403-40cd-8b2d-a7caf28e5bf3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Training Components\n",
        "Here, define any necessary components that you need to train your model, such as the model architecture, the loss function, and the optimizer."
      ],
      "metadata": {
        "id": "2oGRC8Mk0ytJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/kams_colab/mimic_train_labels_pruned.csv\")\n",
        "img_paths = data[\"image_path\"].to_numpy()\n",
        "labels = data[\"positioning\"].to_numpy()\n",
        "distances = data[\"measures\"].to_numpy()\n",
        "\n",
        "dataset = ChestXRayDataset(img_paths=img_paths, labels=labels, distances=distances)\n",
        "\n",
        "def get_train_val_split(dataset, batch_size=10, train_prop=0.8):\n",
        "    dataset_length = len(dataset)\n",
        "    train_length = int(dataset_length * train_prop)\n",
        "    val_length = dataset_length - train_length\n",
        "    train_dataset, val_dataset = random_split(\n",
        "            dataset, [train_length, val_length]\n",
        "        )\n",
        "\n",
        "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    return train_loader, val_loader\n",
        "\n",
        "train_loader, val_loader = get_train_val_split(dataset, batch_size=32, train_prop=0.8)\n",
        "\n",
        "print(len(train_loader.dataset))\n",
        "print(len(val_loader.dataset))\n"
      ],
      "metadata": {
        "id": "-HeB-_-k0x_S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77824db1-d377-4fad-c041-824a2362c47b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9252\n",
            "2313\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Code\n",
        "We provide starter code below that implements a simple training loop in PyTorch. Feel free to modify as you see fit."
      ],
      "metadata": {
        "id": "TXvOaNPB1OGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
        "\n",
        "def calculate_scores(y_true, y_pred):\n",
        "  #  y_pred = y_pred.flatten()\n",
        "  #  locs_positive = np.where(y_pred > 0.5)\n",
        "  #  y_pred = np.zeros(y_pred.shape[0])\n",
        "  #  y_pred[locs_positive] = 1\n",
        "   return f1_score(y_true, y_pred), accuracy_score(y_true, y_pred)\n",
        "\n",
        "def validate(model, loss_fn, val_loader):\n",
        "\n",
        "    f1_scores, acc_scores = [], []\n",
        "    total_loss = 0\n",
        "    for data in tqdm(val_loader):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            inputs = data[\"img\"].to(device)\n",
        "            labels = data[\"labels\"].type(torch.LongTensor).to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            loss_val = loss_fn(outputs.float(), labels)\n",
        "            total_loss += loss_val.item()\n",
        "\n",
        "        f1, acc = calculate_scores(labels.detach().cpu().numpy(), preds.detach().cpu().numpy())\n",
        "        f1_scores.append(f1)\n",
        "        acc_scores.append(acc)\n",
        "    return np.mean(f1_scores), np.mean(acc_scores), total_loss\n",
        "\n",
        "def train(model, loss_fn, train_loader, opt):\n",
        "    f1_scores, rocauc_scores, acc_scores = [], [], []\n",
        "    total_loss = 0\n",
        "    for data in tqdm(train_loader):\n",
        "        model.train()\n",
        "        inputs = data[\"img\"].to(device)\n",
        "        labels = data[\"labels\"].type(torch.LongTensor).to(device)\n",
        "        opt.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        loss_val = loss_fn(outputs.float(), labels)\n",
        "        total_loss += loss_val.item()\n",
        "        loss_val.backward()\n",
        "        opt.step()\n",
        "\n",
        "        f1, acc = calculate_scores(labels.detach().cpu().numpy(), preds.detach().cpu().numpy())\n",
        "        f1_scores.append(f1)\n",
        "        acc_scores.append(acc)\n",
        "    return np.mean(f1_scores), np.mean(acc_scores), total_loss\n",
        "\n",
        "def batch_progress(epoch, tr_f1, tr_acc, tr_loss, val_f1, val_acc, val_loss):\n",
        "    # Batch train data\n",
        "    print(f\"Epoch {epoch} Training Statistics\")\n",
        "    print(f\"F1 Score: {tr_f1}\\n Accuracy: {tr_acc}\\n Loss: {tr_loss}\\n\")\n",
        "    # Batch validation data\n",
        "    print(f\"Epoch {epoch} Validation Statistics\")\n",
        "    print(f\"F1 Score: {val_f1}\\n Accuracy: {val_acc}\\n Loss: {val_loss}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "pWnzKime0exc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xOyDDffJ_nOo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model definition\n",
        "import torch.nn as nn\n",
        "# Load resnet-50 here\n",
        "\n",
        "# FineTuning Architecture\n",
        "# From https://link.springer.com/article/10.1007/s10278-017-9980-7\n",
        "\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "# We have resnet -- outputs 1000 things. But, we only want to output ONE.\n",
        "\n",
        "# We only output one thing.\n",
        "\n",
        "# Why? We want to output the PROBABILITY that a particular image has an ETT device with good positioning.\n",
        "# The probability will be between 0 and 1. We can apply the sigmoid function to the output to receive a probability that a particular\n",
        "# image has an ETT device in good poisitioning. We can use a threshold to determine TRUE or FALSE.\n",
        "# A good starting threshold is if it is less than or equal to 0.5, it is FALSE\n",
        "# otherwise, it is TRUE.\n",
        "\n",
        "def remove_keys(d):\n",
        "  for key in list(d.keys()):\n",
        "    if 'module.jigsaw' in key or 'module.head_jig' in key:\n",
        "      print('warning, jigsaw stream in model')\n",
        "      d.pop(key)\n",
        "    elif 'projection' in key or 'prototypes' in key or 'fc' in key or 'linear' in key or 'head' in key:\n",
        "      print(f'removed {key}')\n",
        "      d.pop(key)\n",
        "  return d\n",
        "\n",
        "\n",
        "def get_model(num_classes):\n",
        "    model = models.resnet50(pretrained=False)\n",
        "    n_features = model.fc.in_features\n",
        "    state_dict = torch.load(\"/content/drive/MyDrive/kams_colab/ImageNet_chestxray14.pth.tar\")\n",
        "    del model.fc\n",
        "    if 'state_dict' in state_dict:\n",
        "      state_dict = state_dict['state_dict']\n",
        "    elif 'model' in state_dict:\n",
        "        state_dict = state_dict['model']\n",
        "    elif 'resnet' in state_dict:\n",
        "        state_dict = state_dict['resnet']\n",
        "\n",
        "    state_dict = remove_keys(state_dict)\n",
        "    model.load_state_dict(state_dict)\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Sequential(\n",
        "        nn.Linear(n_features, n_features),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(n_features, n_features),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(n_features, num_classes),\n",
        "      )\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def save_model(model, epoch, optimizer, val_f1, val_acc, val_loss,\n",
        "               tr_f1, tr_acc, tr_loss, file_name):\n",
        "  print(\"Saving model checkpoint at epoch\", epoch)\n",
        "  torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_f1': val_f1,\n",
        "            'val_acc': val_acc,\n",
        "            'val_loss': val_loss,\n",
        "            'train_f1': tr_f1,\n",
        "            'train_acc': tr_acc,\n",
        "            'train_loss': tr_loss,\n",
        "            }, f\"/content/drive/MyDrive/kams_colab/{file_name}.pth\")\n",
        "\n",
        "model = get_model(num_classes=2).to(device)\n",
        "\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "yWl9WNpX454H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6739dacf-0783-4ed8-c7f4-dae02e5daa5b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "removed fc.0.weight\n",
            "removed fc.0.bias\n",
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Sequential(\n",
            "    (0): Sequential(\n",
            "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "      (1): Dropout(p=0.5, inplace=False)\n",
            "      (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "      (3): ReLU()\n",
            "      (4): Dropout(p=0.5, inplace=False)\n",
            "      (5): Linear(in_features=2048, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.nn as nn\n",
        "# import numpy as np\n",
        "\n",
        "# opt = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
        "# loss_fn = nn.CrossEntropyLoss()\n",
        "# CHECKPOINTS = [25]\n",
        "\n",
        "# EXPERIMENT_NAME = \"chestxray14_tl_exp_3\"\n",
        "# NUM_EPOCHS = 75\n",
        "\n",
        "# train_f1, train_acc, train_loss = [], [], []\n",
        "# val_f1, val_acc, val_loss = [], [], []\n",
        "# best_model_metrics = {\"best_val_acc\": 0.0,\n",
        "#                       \"best_val_f1\": 0.0,\n",
        "#                       \"best_val_loss\":0.0}\n",
        "\n",
        "# for epoch in range(NUM_EPOCHS):\n",
        "#     batch_tr_f1, batch_tr_acc, batch_tr_loss = train(model, loss_fn, train_loader, opt)\n",
        "#     batch_val_f1, batch_val_acc, batch_val_loss = validate(model, loss_fn, val_loader)\n",
        "\n",
        "#     batch_progress(\n",
        "#         epoch, batch_tr_f1, batch_tr_acc, batch_tr_loss,\n",
        "#         batch_val_f1, batch_val_acc, batch_val_loss\n",
        "#         )\n",
        "\n",
        "#     train_f1.append(batch_tr_f1)\n",
        "#     train_acc.append(batch_tr_acc)\n",
        "#     train_loss.append(batch_tr_loss)\n",
        "\n",
        "#     val_f1.append(batch_val_f1)\n",
        "#     val_acc.append(batch_val_acc)\n",
        "#     train_loss.append(batch_val_loss)\n",
        "\n",
        "#     if best_model_metrics[\"best_val_acc\"] < batch_val_acc :\n",
        "#       best_model_metrics[\"best_val_acc\"] = batch_val_acc\n",
        "#       best_model_metrics[\"best_val_f1\"] = batch_val_f1\n",
        "#       best_model_metrics[\"best_val_loss\"] = batch_val_loss\n",
        "#       save_model(\n",
        "#            model, epoch, opt, val_f1, val_acc, val_loss,\n",
        "#            train_f1, train_acc, train_loss,\n",
        "#            f\"{EXPERIMENT_NAME}_epoch_{epoch}\"\n",
        "#           )\n",
        "#     print(f'Your best model has \\n \\\n",
        "#         Val Acc: {best_model_metrics[\"best_val_acc\"]} \\n \\\n",
        "#         Val F1: {best_model_metrics[\"best_val_f1\"]}')\n",
        "\n",
        "# ## TRAINING COMPLETE ##\n",
        "# save_model(\n",
        "#            model, epoch, opt, val_f1, val_acc, val_loss,\n",
        "#            train_f1, train_acc, train_loss,\n",
        "#            f\"{EXPERIMENT_NAME}_epoch_{NUM_EPOCHS}\"\n",
        "#           )\n",
        "\n",
        "# train_f1 = np.array(train_f1)\n",
        "# train_acc = np.array(train_acc)\n",
        "# train_loss = np.array(train_loss)\n",
        "\n",
        "# val_f1 = np.array(val_f1)\n",
        "# val_acc = np.array(val_acc)\n",
        "# val_loss = np.array(val_loss)"
      ],
      "metadata": {
        "id": "Y4xoCnObGI6D"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# kafhgkahdfgkhdfg"
      ],
      "metadata": {
        "id": "TSWQR0iGZd6_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Plot loss\n",
        "# plt.plot(train_loss, label=\"Train Loss\")\n",
        "# plt.plot(val_loss, label=\"Val Loss\")\n",
        "# plt.xlabel(\"Epoch\")\n",
        "# plt.ylabel(\"Loss\")\n",
        "# plt.title('Epoch vs Loss')\n",
        "# plt.show()\n",
        "\n",
        "# # Plot f1\n",
        "# plt.plot(train_f1, label=\"Train F1\")\n",
        "# plt.plot(val_f1, label=\"Val F1\")\n",
        "# plt.xlabel(\"Epoch\")\n",
        "# plt.ylabel(\"F1 Score\")\n",
        "# plt.title('Epoch vs F1 Score')\n",
        "# plt.show()\n",
        "\n",
        "# # Plot Accuracy\n",
        "# plt.plot(train_f1, label=\"Train Acc\")\n",
        "# plt.plot(val_f1, label=\"Val Acc\")\n",
        "# plt.xlabel(\"Epoch\")\n",
        "# plt.ylabel(\"Accuracy\")\n",
        "# plt.title('Epoch vs Accuracy')\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "XYQxUM5Jb6RF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submitting Your Results\n",
        "Once you have successfully trained your model, generate predictions on the test set and save your results as a `.csv` file. This file can then be uploaded to the leaderboard.\n",
        "\n",
        "Your final `.csv` file **must** have the following format:\n",
        "- There must be a column titled `image_path` with the paths to the test set images. This column should be identical to the one provided in `mimic_test_student.csv`.\n",
        "- There must be a column titled `pred` with your model outputs.\n",
        "  - If you are running the `distance categorization` task, this column must have floating point numbers ranging between 0 and 1. Higher numbers should indicate a greater likelihood that the tube distance is abnormal. Hint: You can convert model outputs to the 0 to 1 range by applying the sigmoid activation function (torch.nn.sigmoid())\n",
        "  - If you are running the `distance prediction` task, this column must have numbers representing the tube distance in centimeters.\n",
        "- Double check that there are 500 rows in your output file"
      ],
      "metadata": {
        "id": "l8F_1wzY6I7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet50_chestxray14 = models.resnet50(pretrained=False)\n",
        "n_features = resnet50_chestxray14.fc.in_features\n",
        "state_dict = torch.load(\"/content/drive/MyDrive/kams_colab/chestxray14_tl_exp_3_epoch_8.pth\")\n",
        "\n",
        "resnet50_chestxray14.fc = nn.Sequential(\n",
        "    nn.Sequential(\n",
        "    nn.Linear(n_features, n_features),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(n_features, n_features),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(n_features, 2),\n",
        "  )\n",
        ")\n",
        "resnet50_chestxray14.load_state_dict(state_dict[\"model_state_dict\"])\n",
        "resnet50_chestxray14.to(device).eval()\n",
        "\n",
        "\n",
        "resnet50_exp5 = models.resnet50(weights=\"ResNet50_Weights.DEFAULT\")\n",
        "n_features = resnet50_exp5.fc.in_features\n",
        "resnet50_exp5.fc = nn.Sequential(\n",
        "    nn.Linear(n_features, n_features),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(n_features, 1),\n",
        ")\n",
        "\n",
        "for i, child in enumerate(resnet50_exp5.children()):\n",
        "  if i == 9:\n",
        "    for param in child.parameters():\n",
        "        param.requires_grad = True\n",
        "    break\n",
        "  for param in child.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "resnet50_exp6 = models.resnet50(weights=\"ResNet50_Weights.DEFAULT\")\n",
        "n_features = resnet50_exp6.fc.in_features\n",
        "resnet50_exp6.fc = nn.Sequential(\n",
        "    nn.Linear(n_features, n_features),\n",
        "    nn.Dropout(p=0.5),\n",
        "    nn.Linear(n_features, n_features),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(n_features, 1),\n",
        ")\n",
        "\n",
        "for i, child in enumerate(resnet50_exp6.children()):\n",
        "  for param in child.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(state_dict[\"val_acc\"][-1])\n",
        "print(state_dict['val_f1'][-1])\n",
        "\n",
        "model_exp5 = torch.load(\"/content/drive/MyDrive/kams_colab/resnet50_tl_exp_5_epoch_6.pth\")\n",
        "resnet50_exp5.load_state_dict(model_exp5[\"model_state_dict\"])\n",
        "print(model_exp5[\"val_acc\"][-1])\n",
        "print(model_exp5['val_f1'][-1])\n",
        "resnet50_exp5.to(device).eval()\n",
        "model_exp6 = torch.load(\"/content/drive/MyDrive/kams_colab/resnet50_tl_exp_6_epoch_28.pth\")\n",
        "resnet50_exp6.load_state_dict(model_exp6[\"model_state_dict\"])\n",
        "print(model_exp6[\"val_acc\"][-1])\n",
        "print(model_exp6['val_f1'][-1])\n",
        "resnet50_exp6.to(device).eval()\n",
        "\n",
        "test_set = pd.read_csv(\"/content/drive/MyDrive/kams_colab/mimic_test_student.csv\")\n",
        "test_set = test_set.drop(columns=[\"Unnamed: 0\", \"split\", \"patient_id\", \"study_id\", \"image_id\"])\n",
        "\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, img_path):\n",
        "        super(TestDataset, self).__init__()\n",
        "        self.img_paths = img_paths\n",
        "        # Fill in __init__() here\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        # Fill in __len__() here\n",
        "        return self.img_paths.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        out_dict = {\"idx\": torch.tensor(idx),}\n",
        "\n",
        "        # Fill in __getitem__() here\n",
        "        im = Image.open(f\"/content/{self.img_paths[idx]}\")\n",
        "\n",
        "        w, h = im.size\n",
        "        ima = Image.new('RGB', (w,h))\n",
        "        data = zip(im.getdata(), im.getdata(), im.getdata())\n",
        "        ima.putdata(list(data))\n",
        "        convert_tensor = transforms.Compose([\n",
        "            transforms.Resize(size=224),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "        img_as_tensor = convert_tensor(ima)\n",
        "        img_as_tensor.requires_grad_ = True\n",
        "        out_dict[\"img\"] = img_as_tensor\n",
        "\n",
        "        return out_dict\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M5osagFf5Fox",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20d3ed6d-d81e-47fb-9669-3c739cb65fba"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7842881944444444\n",
            "0.8572551467124184\n",
            "0.7447916666666666\n",
            "0.8462208445837909\n",
            "0.7725694444444444\n",
            "0.8453939954955793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "test_image_paths = test_set[\"image_path\"].to_numpy()\n",
        "# test_dataset = TestDataset(img_path=test_image_paths)\n",
        "# test_loader = DataLoader(dataset=test_dataset, batch_size=25, shuffle=True, drop_last=False)\n",
        "inputs = []\n",
        "def transform_image(path):\n",
        "   # Fill in __getitem__() here\n",
        "    im = Image.open(f\"/content/{path}\")\n",
        "\n",
        "    w, h = im.size\n",
        "    ima = Image.new('RGB', (w,h))\n",
        "    data = zip(im.getdata(), im.getdata(), im.getdata())\n",
        "    ima.putdata(list(data))\n",
        "    convert_tensor = transforms.Compose([\n",
        "        transforms.Resize(size=224),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    img_as_tensor = convert_tensor(ima)\n",
        "    img_as_tensor.requires_grad_ = True\n",
        "    return img_as_tensor\n",
        "\n",
        "def handle_two_outputs(output):\n",
        "  output = F.softmax(output, dim=1)\n",
        "  maxs, idxs = torch.max(output, dim=1)\n",
        "  output = torch.flatten(output)\n",
        "  output = output.detach().cpu().numpy()\n",
        "  idxs = idxs.detach().cpu().numpy()\n",
        "  if idxs[0] == 1:\n",
        "    return output[1]\n",
        "  return 1 - output[0]\n",
        "preds = []\n",
        "for path in test_image_paths:\n",
        "  input = transform_image(path).to(device)\n",
        "  input = input.unsqueeze(0).to(device)\n",
        "  # out1 = resnet50_exp5(input)\n",
        "  out2 = resnet50_exp6(input)\n",
        "  out3 = resnet50_chestxray14(input)\n",
        "  out3 = handle_two_outputs(out3)\n",
        "  # prob1 = torch.sigmoid(out1[0]).detach().cpu().numpy()\n",
        "  prob2 = torch.sigmoid(out2[0]).detach().cpu().numpy()[0]\n",
        "  prob3 = out3\n",
        "  avg = (prob3 + prob2) / 2\n",
        "  # res = res.detach().cpu().numpy()\n",
        "  preds.append(1 - avg)\n",
        "  print(preds)\n",
        "\n",
        "test_set[\"pred\"] = preds\n",
        "\n",
        "test_set\n"
      ],
      "metadata": {
        "id": "oAB4AZUz8_QR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b04a73f1-93b1-4bb1-ce43-bf67106b638b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.4808625280857086]\n",
            "[0.4808625280857086, 0.20278513431549072]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625, 5.960464477539062e-07]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625, 5.960464477539062e-07, 0.03078216314315796]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625, 5.960464477539062e-07, 0.03078216314315796, 0.007481694221496582]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625, 5.960464477539062e-07, 0.03078216314315796, 0.007481694221496582, 0.0]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625, 5.960464477539062e-07, 0.03078216314315796, 0.007481694221496582, 0.0, 0.6273803114891052]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625, 5.960464477539062e-07, 0.03078216314315796, 0.007481694221496582, 0.0, 0.6273803114891052, 0.008691191673278809]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625, 5.960464477539062e-07, 0.03078216314315796, 0.007481694221496582, 0.0, 0.6273803114891052, 0.008691191673278809, 0.0029903650283813477]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625, 5.960464477539062e-07, 0.03078216314315796, 0.007481694221496582, 0.0, 0.6273803114891052, 0.008691191673278809, 0.0029903650283813477, 0.23525774478912354]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625, 5.960464477539062e-07, 0.03078216314315796, 0.007481694221496582, 0.0, 0.6273803114891052, 0.008691191673278809, 0.0029903650283813477, 0.23525774478912354, 5.799531936645508e-05]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625, 5.960464477539062e-07, 0.03078216314315796, 0.007481694221496582, 0.0, 0.6273803114891052, 0.008691191673278809, 0.0029903650283813477, 0.23525774478912354, 5.799531936645508e-05, 0.9910484724678099]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625, 5.960464477539062e-07, 0.03078216314315796, 0.007481694221496582, 0.0, 0.6273803114891052, 0.008691191673278809, 0.0029903650283813477, 0.23525774478912354, 5.799531936645508e-05, 0.9910484724678099, 1.1920928955078125e-07]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625, 5.960464477539062e-07, 0.03078216314315796, 0.007481694221496582, 0.0, 0.6273803114891052, 0.008691191673278809, 0.0029903650283813477, 0.23525774478912354, 5.799531936645508e-05, 0.9910484724678099, 1.1920928955078125e-07, 0.003541231155395508]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625, 5.960464477539062e-07, 0.03078216314315796, 0.007481694221496582, 0.0, 0.6273803114891052, 0.008691191673278809, 0.0029903650283813477, 0.23525774478912354, 5.799531936645508e-05, 0.9910484724678099, 1.1920928955078125e-07, 0.003541231155395508, 7.456541061401367e-05]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625, 5.960464477539062e-07, 0.03078216314315796, 0.007481694221496582, 0.0, 0.6273803114891052, 0.008691191673278809, 0.0029903650283813477, 0.23525774478912354, 5.799531936645508e-05, 0.9910484724678099, 1.1920928955078125e-07, 0.003541231155395508, 7.456541061401367e-05, 0.006358504295349121]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625, 5.960464477539062e-07, 0.03078216314315796, 0.007481694221496582, 0.0, 0.6273803114891052, 0.008691191673278809, 0.0029903650283813477, 0.23525774478912354, 5.799531936645508e-05, 0.9910484724678099, 1.1920928955078125e-07, 0.003541231155395508, 7.456541061401367e-05, 0.006358504295349121, 0.5620200037956238]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625, 5.960464477539062e-07, 0.03078216314315796, 0.007481694221496582, 0.0, 0.6273803114891052, 0.008691191673278809, 0.0029903650283813477, 0.23525774478912354, 5.799531936645508e-05, 0.9910484724678099, 1.1920928955078125e-07, 0.003541231155395508, 7.456541061401367e-05, 0.006358504295349121, 0.5620200037956238, 0.009395480155944824]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625, 5.960464477539062e-07, 0.03078216314315796, 0.007481694221496582, 0.0, 0.6273803114891052, 0.008691191673278809, 0.0029903650283813477, 0.23525774478912354, 5.799531936645508e-05, 0.9910484724678099, 1.1920928955078125e-07, 0.003541231155395508, 7.456541061401367e-05, 0.006358504295349121, 0.5620200037956238, 0.009395480155944824, 0.014282584190368652]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625, 5.960464477539062e-07, 0.03078216314315796, 0.007481694221496582, 0.0, 0.6273803114891052, 0.008691191673278809, 0.0029903650283813477, 0.23525774478912354, 5.799531936645508e-05, 0.9910484724678099, 1.1920928955078125e-07, 0.003541231155395508, 7.456541061401367e-05, 0.006358504295349121, 0.5620200037956238, 0.009395480155944824, 0.014282584190368652, 1.2516975402832031e-06]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625, 5.960464477539062e-07, 0.03078216314315796, 0.007481694221496582, 0.0, 0.6273803114891052, 0.008691191673278809, 0.0029903650283813477, 0.23525774478912354, 5.799531936645508e-05, 0.9910484724678099, 1.1920928955078125e-07, 0.003541231155395508, 7.456541061401367e-05, 0.006358504295349121, 0.5620200037956238, 0.009395480155944824, 0.014282584190368652, 1.2516975402832031e-06, 0.7191101312637329]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625, 5.960464477539062e-07, 0.03078216314315796, 0.007481694221496582, 0.0, 0.6273803114891052, 0.008691191673278809, 0.0029903650283813477, 0.23525774478912354, 5.799531936645508e-05, 0.9910484724678099, 1.1920928955078125e-07, 0.003541231155395508, 7.456541061401367e-05, 0.006358504295349121, 0.5620200037956238, 0.009395480155944824, 0.014282584190368652, 1.2516975402832031e-06, 0.7191101312637329, 0.4308999180793762]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625, 5.960464477539062e-07, 0.03078216314315796, 0.007481694221496582, 0.0, 0.6273803114891052, 0.008691191673278809, 0.0029903650283813477, 0.23525774478912354, 5.799531936645508e-05, 0.9910484724678099, 1.1920928955078125e-07, 0.003541231155395508, 7.456541061401367e-05, 0.006358504295349121, 0.5620200037956238, 0.009395480155944824, 0.014282584190368652, 1.2516975402832031e-06, 0.7191101312637329, 0.4308999180793762, 0.0011563301086425781]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625, 5.960464477539062e-07, 0.03078216314315796, 0.007481694221496582, 0.0, 0.6273803114891052, 0.008691191673278809, 0.0029903650283813477, 0.23525774478912354, 5.799531936645508e-05, 0.9910484724678099, 1.1920928955078125e-07, 0.003541231155395508, 7.456541061401367e-05, 0.006358504295349121, 0.5620200037956238, 0.009395480155944824, 0.014282584190368652, 1.2516975402832031e-06, 0.7191101312637329, 0.4308999180793762, 0.0011563301086425781, 0.005237102508544922]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625, 5.960464477539062e-07, 0.03078216314315796, 0.007481694221496582, 0.0, 0.6273803114891052, 0.008691191673278809, 0.0029903650283813477, 0.23525774478912354, 5.799531936645508e-05, 0.9910484724678099, 1.1920928955078125e-07, 0.003541231155395508, 7.456541061401367e-05, 0.006358504295349121, 0.5620200037956238, 0.009395480155944824, 0.014282584190368652, 1.2516975402832031e-06, 0.7191101312637329, 0.4308999180793762, 0.0011563301086425781, 0.005237102508544922, 0.0027063488960266113]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625, 5.960464477539062e-07, 0.03078216314315796, 0.007481694221496582, 0.0, 0.6273803114891052, 0.008691191673278809, 0.0029903650283813477, 0.23525774478912354, 5.799531936645508e-05, 0.9910484724678099, 1.1920928955078125e-07, 0.003541231155395508, 7.456541061401367e-05, 0.006358504295349121, 0.5620200037956238, 0.009395480155944824, 0.014282584190368652, 1.2516975402832031e-06, 0.7191101312637329, 0.4308999180793762, 0.0011563301086425781, 0.005237102508544922, 0.0027063488960266113, 0.37953853607177734]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625, 5.960464477539062e-07, 0.03078216314315796, 0.007481694221496582, 0.0, 0.6273803114891052, 0.008691191673278809, 0.0029903650283813477, 0.23525774478912354, 5.799531936645508e-05, 0.9910484724678099, 1.1920928955078125e-07, 0.003541231155395508, 7.456541061401367e-05, 0.006358504295349121, 0.5620200037956238, 0.009395480155944824, 0.014282584190368652, 1.2516975402832031e-06, 0.7191101312637329, 0.4308999180793762, 0.0011563301086425781, 0.005237102508544922, 0.0027063488960266113, 0.37953853607177734, 0.06306189298629761]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625, 5.960464477539062e-07, 0.03078216314315796, 0.007481694221496582, 0.0, 0.6273803114891052, 0.008691191673278809, 0.0029903650283813477, 0.23525774478912354, 5.799531936645508e-05, 0.9910484724678099, 1.1920928955078125e-07, 0.003541231155395508, 7.456541061401367e-05, 0.006358504295349121, 0.5620200037956238, 0.009395480155944824, 0.014282584190368652, 1.2516975402832031e-06, 0.7191101312637329, 0.4308999180793762, 0.0011563301086425781, 0.005237102508544922, 0.0027063488960266113, 0.37953853607177734, 0.06306189298629761, 0.019539177417755127]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625, 5.960464477539062e-07, 0.03078216314315796, 0.007481694221496582, 0.0, 0.6273803114891052, 0.008691191673278809, 0.0029903650283813477, 0.23525774478912354, 5.799531936645508e-05, 0.9910484724678099, 1.1920928955078125e-07, 0.003541231155395508, 7.456541061401367e-05, 0.006358504295349121, 0.5620200037956238, 0.009395480155944824, 0.014282584190368652, 1.2516975402832031e-06, 0.7191101312637329, 0.4308999180793762, 0.0011563301086425781, 0.005237102508544922, 0.0027063488960266113, 0.37953853607177734, 0.06306189298629761, 0.019539177417755127, 0.40955233573913574]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625, 5.960464477539062e-07, 0.03078216314315796, 0.007481694221496582, 0.0, 0.6273803114891052, 0.008691191673278809, 0.0029903650283813477, 0.23525774478912354, 5.799531936645508e-05, 0.9910484724678099, 1.1920928955078125e-07, 0.003541231155395508, 7.456541061401367e-05, 0.006358504295349121, 0.5620200037956238, 0.009395480155944824, 0.014282584190368652, 1.2516975402832031e-06, 0.7191101312637329, 0.4308999180793762, 0.0011563301086425781, 0.005237102508544922, 0.0027063488960266113, 0.37953853607177734, 0.06306189298629761, 0.019539177417755127, 0.40955233573913574, 0.6381421089172363]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625, 5.960464477539062e-07, 0.03078216314315796, 0.007481694221496582, 0.0, 0.6273803114891052, 0.008691191673278809, 0.0029903650283813477, 0.23525774478912354, 5.799531936645508e-05, 0.9910484724678099, 1.1920928955078125e-07, 0.003541231155395508, 7.456541061401367e-05, 0.006358504295349121, 0.5620200037956238, 0.009395480155944824, 0.014282584190368652, 1.2516975402832031e-06, 0.7191101312637329, 0.4308999180793762, 0.0011563301086425781, 0.005237102508544922, 0.0027063488960266113, 0.37953853607177734, 0.06306189298629761, 0.019539177417755127, 0.40955233573913574, 0.6381421089172363, 0.10370659828186035]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625, 5.960464477539062e-07, 0.03078216314315796, 0.007481694221496582, 0.0, 0.6273803114891052, 0.008691191673278809, 0.0029903650283813477, 0.23525774478912354, 5.799531936645508e-05, 0.9910484724678099, 1.1920928955078125e-07, 0.003541231155395508, 7.456541061401367e-05, 0.006358504295349121, 0.5620200037956238, 0.009395480155944824, 0.014282584190368652, 1.2516975402832031e-06, 0.7191101312637329, 0.4308999180793762, 0.0011563301086425781, 0.005237102508544922, 0.0027063488960266113, 0.37953853607177734, 0.06306189298629761, 0.019539177417755127, 0.40955233573913574, 0.6381421089172363, 0.10370659828186035, 0.7101760506629944]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625, 5.960464477539062e-07, 0.03078216314315796, 0.007481694221496582, 0.0, 0.6273803114891052, 0.008691191673278809, 0.0029903650283813477, 0.23525774478912354, 5.799531936645508e-05, 0.9910484724678099, 1.1920928955078125e-07, 0.003541231155395508, 7.456541061401367e-05, 0.006358504295349121, 0.5620200037956238, 0.009395480155944824, 0.014282584190368652, 1.2516975402832031e-06, 0.7191101312637329, 0.4308999180793762, 0.0011563301086425781, 0.005237102508544922, 0.0027063488960266113, 0.37953853607177734, 0.06306189298629761, 0.019539177417755127, 0.40955233573913574, 0.6381421089172363, 0.10370659828186035, 0.7101760506629944, 0.17761623859405518]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625, 5.960464477539062e-07, 0.03078216314315796, 0.007481694221496582, 0.0, 0.6273803114891052, 0.008691191673278809, 0.0029903650283813477, 0.23525774478912354, 5.799531936645508e-05, 0.9910484724678099, 1.1920928955078125e-07, 0.003541231155395508, 7.456541061401367e-05, 0.006358504295349121, 0.5620200037956238, 0.009395480155944824, 0.014282584190368652, 1.2516975402832031e-06, 0.7191101312637329, 0.4308999180793762, 0.0011563301086425781, 0.005237102508544922, 0.0027063488960266113, 0.37953853607177734, 0.06306189298629761, 0.019539177417755127, 0.40955233573913574, 0.6381421089172363, 0.10370659828186035, 0.7101760506629944, 0.17761623859405518, 0.00013911724090576172]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625, 5.960464477539062e-07, 0.03078216314315796, 0.007481694221496582, 0.0, 0.6273803114891052, 0.008691191673278809, 0.0029903650283813477, 0.23525774478912354, 5.799531936645508e-05, 0.9910484724678099, 1.1920928955078125e-07, 0.003541231155395508, 7.456541061401367e-05, 0.006358504295349121, 0.5620200037956238, 0.009395480155944824, 0.014282584190368652, 1.2516975402832031e-06, 0.7191101312637329, 0.4308999180793762, 0.0011563301086425781, 0.005237102508544922, 0.0027063488960266113, 0.37953853607177734, 0.06306189298629761, 0.019539177417755127, 0.40955233573913574, 0.6381421089172363, 0.10370659828186035, 0.7101760506629944, 0.17761623859405518, 0.00013911724090576172, 0.6061039865016937]\n",
            "[0.4808625280857086, 0.20278513431549072, 0.04067671298980713, 0.20239078998565674, 0.0698930025100708, 1.704692840576172e-05, 0.015727341175079346, 2.5928020477294922e-05, 0.00012087821960449219, 0.002567291259765625, 5.960464477539062e-07, 0.03078216314315796, 0.007481694221496582, 0.0, 0.6273803114891052, 0.008691191673278809, 0.0029903650283813477, 0.23525774478912354, 5.799531936645508e-05, 0.9910484724678099, 1.1920928955078125e-07, 0.003541231155395508, 7.456541061401367e-05, 0.006358504295349121, 0.5620200037956238, 0.009395480155944824, 0.014282584190368652, 1.2516975402832031e-06, 0.7191101312637329, 0.4308999180793762, 0.0011563301086425781, 0.005237102508544922, 0.0027063488960266113, 0.37953853607177734, 0.06306189298629761, 0.019539177417755127, 0.40955233573913574, 0.6381421089172363, 0.10370659828186035, 0.7101760506629944, 0.17761623859405518, 0.00013911724090576172, 0.6061039865016937, 0.5074424743652344]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-b9cc884f3edf>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_image_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m   \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m   \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0;31m# out1 = resnet50_exp5(input)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-b9cc884f3edf>\u001b[0m in \u001b[0;36mtransform_image\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mima\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mima\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     convert_tensor = transforms.Compose([\n\u001b[1;32m     15\u001b[0m         \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_set.to_csv(f\"/content/drive/MyDrive/kams_colab/test_results_exp6_and_xray.csv\")"
      ],
      "metadata": {
        "id": "q-6CgEWNWHdl"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RuRWCSiVXL6n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}