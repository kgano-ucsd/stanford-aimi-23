{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMfGLGNLnXk7"
      },
      "source": [
        "## AIMI High School Internship 2023\n",
        "### Notebook 2: Training a Vision Model to Predict ET Distances\n",
        "\n",
        "**The Problem**: Given a chest X-ray, our goal in this project is to predict the distance from an endotracheal tube to the carina. This is an important clinical task - endotracheal tubes that are positioned too far (>5cm) above the carina will not work effectively.\n",
        "\n",
        "**Your Second Task**: You should now have a training dataset consisting of (a) chest X-rays and (b) annotations indicating the distance of the endotracheal tube from the carina. Now, your goal is to train a computer vision model to predict endotracheal tube distance from the image. You have **two options** for this task, and you may attempt one or both of these:\n",
        "- *Distance Categorization* : Train a model to determine whether the position of a tube is abnormal (>5.0 cm) or normal (â‰¤ 5.0 cm).\n",
        "- *Distance Prediction*: Train a model that predicts the distance of the endotracheal tube from the carina in centimeters.\n",
        "\n",
        "In this notebook, we provide some simple starter code to get you started on training a computer vision model. You are not required to use this template - feel free to modify as you see fit.\n",
        "\n",
        "**Submitting Your Model**: We have created a leaderboard where you can submit your model and view results on the held-out test set. We provide instructions below for submitting your model to the leaderboard. **Please follow these directions carefully**.\n",
        "\n",
        "We will evaluate your results on the held-out test set with the following evaluation metrics:\n",
        "- *Distance Categorization* : We will measure AUROC, which is a metric commonly used in healthcare tasks. See this blog for a good explanation of AUROC: https://glassboxmedicine.com/2019/02/23/measuring-performance-auc-auroc/\n",
        "- *Distance Prediction*: We will measure the mean average error (also known as L1 distance) between the predicted distances and the true distances.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ1rTMrgpoil"
      },
      "source": [
        "## Load Data\n",
        "Before you begin, make sure to go to `Runtime` > `Change Runtime Type` and select a T4 GPU. Then, upload `data.zip`. It should take about 10 minutes for these files to be uploaded. Then, run the following cells to unzip the dataset (which should take < 10 seconds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5etX4eYtu_s"
      },
      "source": [
        "## Import Libraries\n",
        "We are leveraging the PyTorch framework to train our models. For more information and tutorials on PyTorch, see this link: https://pytorch.org/tutorials/beginner/basics/intro.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzhTFDi7tuPK"
      },
      "outputs": [],
      "source": [
        "# Some libraries that you may find useful are included here.\n",
        "# To import a library that isn't provided with Colab, use the following command: !pip install torchmetrics\n",
        "import torch\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, random_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY7yvIM0yl4M"
      },
      "source": [
        "## Create Dataloaders\n",
        "We will implement a custom Dataset class to load in data. A custom Dataset class must have three methods: `__init__`, which sets up any class variables, `__len__`, which defines the total number of images, and `__getitem__`, which returns a single image and its paired label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "FwH5586UqAnY"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class ChestXRayDataset(Dataset):\n",
        "    def __init__(self, img_paths, labels, distances):\n",
        "        super(ChestXRayDataset, self).__init__()\n",
        "        self.img_paths = img_paths\n",
        "        self.labels = labels\n",
        "        self.distances = distances\n",
        "        # Fill in __init__() here\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        # Fill in __len__() here\n",
        "        return self.labels.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        out_dict = {\"idx\": torch.tensor(idx),}\n",
        "\n",
        "        # Fill in __getitem__() here\n",
        "        img = Image.open(f\"data/{self.img_paths[idx]}\")\n",
        "        convert_tensor = transforms.ToTensor()\n",
        "        img_as_tensor = convert_tensor(img)\n",
        "        img_as_tensor.requires_grad_ = True\n",
        "        out_dict[\"img\"] = img_as_tensor\n",
        "        out_dict[\"distance\"] = self.distances[idx]\n",
        "\n",
        "        return out_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oGRC8Mk0ytJ"
      },
      "source": [
        "## Define Training Components\n",
        "Here, define any necessary components that you need to train your model, such as the model architecture, the loss function, and the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "-HeB-_-k0x_S"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9252\n",
            "2313\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# loss = nn.CrossEntropyLoss()\n",
        "# model = # Model Architecture (make sure to load the model on GPU, not CPU!)\n",
        "# opt = torch.optim.AdamW(model.parameters(), lr=1e-4) # AdamW is a commonly-used optimizer. Feel free to modify.\n",
        "\n",
        "data = pd.read_csv(\"mimic_train_labels_pruned.csv\")\n",
        "img_paths = data[\"image_path\"].to_numpy()\n",
        "labels = data[\"positioning\"].to_numpy()\n",
        "distances = data[\"measures\"].to_numpy()\n",
        "\n",
        "dataset = ChestXRayDataset(img_paths=img_paths, labels=labels, distances=distances)\n",
        "\n",
        "def get_train_val_split(dataset, batch_size=10, train_prop=0.8):\n",
        "    dataset_length = len(dataset)\n",
        "    train_length = int(dataset_length * train_prop)\n",
        "    val_length = dataset_length - train_length\n",
        "    train_dataset, val_dataset = random_split(\n",
        "            dataset, [train_length, val_length]\n",
        "        )\n",
        "\n",
        "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    return train_loader, val_loader\n",
        "\n",
        "train_loader, val_loader = get_train_val_split(dataset, batch_size=32)\n",
        "\n",
        "print(len(train_loader.dataset))\n",
        "print(len(val_loader.dataset))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXvOaNPB1OGH"
      },
      "source": [
        "## Training Code\n",
        "We provide starter code below that implements a simple training loop in PyTorch. Feel free to modify as you see fit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "pWnzKime0exc"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'sklearn'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m f1_score, roc_auc_score, accuracy_score\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculate_scores\u001b[39m(y_true, y_pred):\n\u001b[1;32m      4\u001b[0m    \u001b[39mreturn\u001b[39;00m f1_score(y_true, y_pred), roc_auc_score(y_true, y_pred), accuracy_score(y_true, y_pred) \n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
        "\n",
        "def calculate_scores(y_true, y_pred):\n",
        "   return f1_score(y_true, y_pred), roc_auc_score(y_true, y_pred), accuracy_score(y_true, y_pred) \n",
        "\n",
        "def validate(model, loss_fn, val_loader):\n",
        "    \n",
        "    f1_scores, rocauc_scores, acc_scores = [], [], []\n",
        "    total_loss = 0\n",
        "    for data in tqdm(val_loader):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            inputs = data[\"img\"]\n",
        "            labels = data[\"labels\"]\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            loss_val = loss_fn(preds, labels)\n",
        "            total_loss += loss_val\n",
        "            loss_val.backward()\n",
        "\n",
        "        f1, rocauc, acc = calculate_scores(labels.detach().cpu().numpy(), preds.detach().cpu().numpy())\n",
        "        f1_scores.append(f1)\n",
        "        rocauc_scores.append(rocauc)\n",
        "        acc_scores.append(acc)\n",
        "    return np.mean(f1_scores), np.mean(rocauc_scores), np.mean(acc_scores)\n",
        "\n",
        "def train(model, loss_fn, train_loader, opt):\n",
        "    f1_scores, rocauc_scores, acc_scores = [], [], []\n",
        "    total_loss = 0\n",
        "    for data in tqdm(train_loader):\n",
        "        model.train()\n",
        "        inputs = data[\"img\"]\n",
        "        labels = data[\"labels\"]\n",
        "        opt.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        loss_val = loss_fn(preds, labels)\n",
        "        total_loss += loss_val\n",
        "        loss_val.backward()\n",
        "        opt.step()\n",
        "\n",
        "        f1, rocauc, acc = calculate_scores(labels.detach().cpu().numpy(), preds.detach().cpu().numpy())\n",
        "        f1_scores.append(f1)\n",
        "        rocauc_scores.append(rocauc)\n",
        "        acc_scores.append(acc)\n",
        "    return np.mean(f1_scores), np.mean(rocauc_scores), np.mean(acc_scores), total_loss\n",
        "\n",
        "def batch_progress(epoch, tr_f1, tr_acc, tr_roc, tr_loss, val_f1, val_acc, val_roc, val_loss):\n",
        "    # Batch train data\n",
        "    print(f\"Epoch {epoch} Training Statistics\")\n",
        "    print(f\"F1 Score: {tr_f1}\\n Accuracy: {tr_acc}\\n AUROC: {tr_roc}\\n Loss: {tr_loss}\\n\")\n",
        "    # Batch validation data\n",
        "    print(f\"Epoch {epoch} Validation Statistics\")\n",
        "    print(f\"F1 Score: {val_f1}\\n Accuracy: {val_acc}\\n AUROC: {val_roc}\\n Loss: {val_loss}\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model definition\n",
        "import torch.nn as nn\n",
        "# Load resnet-50 here\n",
        "\n",
        "# FineTuning Architecture\n",
        "# From https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8933872/#b50\n",
        "\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "def get_model(num_classes):\n",
        "    resnet50_aimi = models.resnet50(pretrained=True)\n",
        "    n_features = resnet50_aimi.fc.in_features\n",
        "    try:\n",
        "        # model.fc = nn.Linear(n_features, K)\n",
        "        resnet50_aimi.fc = nn.Sequential(\n",
        "            nn.Linear(n_features, n_features),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(n_features, n_features),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(n_features, num_classes)\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(\"ERROR at: model.fc = nn.Linear(n_features, K)\")\n",
        "        raise e\n",
        "    return resnet50_aimi\n",
        "\n",
        "model = get_model(num_classes=2)\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWl9WNpX454H"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "model = # Define model\n",
        "opt = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "train_f1, train_rocauc, train_acc, train_loss = [], [], [], []\n",
        "val_f1, val_rocauc, val_acc, val_loss = [], [], [], []\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    batch_tr_f1, batch_tr_rocauc, batch_tr_acc, batch_tr_loss = train(model, loss_fn, train_loader, opt)\n",
        "    batch_val_f1, batch_val_rocauc, batch_val_acc, batch_val_loss = validate(model, loss_fn, val_loader)\n",
        "\n",
        "    batch_progress(\n",
        "        batch_tr_f1, batch_tr_acc, batch_tr_rocauc, batch_tr_loss, \n",
        "        batch_val_f1, batch_val_acc, batch_tr_rocauc, batch_val_loss\n",
        "        )\n",
        "    \n",
        "    train_f1.append(batch_tr_f1)\n",
        "    train_rocauc.append(batch_tr_rocauc)\n",
        "    train_acc.append(batch_tr_acc)\n",
        "    train_loss.append(batch_tr_loss)\n",
        "    \n",
        "    val_f1.append(batch_val_f1)\n",
        "    val_rocauc.append(batch_val_rocauc)\n",
        "    val_acc.append(batch_val_acc)\n",
        "    train_loss.append(batch_val_loss)\n",
        "\n",
        "train_f1 = np.array(train_f1)\n",
        "train_rocauc = np.array(train_rocauc)\n",
        "train_acc = np.array(train_acc)\n",
        "train_loss = np.array(train_loss)\n",
        "\n",
        "val_f1 = np.array(val_f1)\n",
        "val_rocauc = np.array(val_rocauc)\n",
        "val_acc = np.array(val_acc)\n",
        "val_loss = np.array(val_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8F_1wzY6I7j"
      },
      "source": [
        "## Submitting Your Results\n",
        "Once you have successfully trained your model, generate predictions on the test set and save your results as a `.csv` file. This file can then be uploaded to the leaderboard.\n",
        "\n",
        "Your final `.csv` file **must** have the following format:\n",
        "- There must be a column titled `image_path` with the paths to the test set images. This column should be identical to the one provided in `mimic_test_student.csv`.\n",
        "- There must be a column titled `pred` with your model outputs.\n",
        "  - If you are running the `distance categorization` task, this column must have floating point numbers ranging between 0 and 1. Higher numbers should indicate a greater likelihood that the tube distance is abnormal. Hint: You can convert model outputs to the 0 to 1 range by applying the sigmoid activation function (torch.nn.sigmoid())\n",
        "  - If you are running the `distance prediction` task, this column must have numbers representing the tube distance in centimeters.\n",
        "- Double check that there are 500 rows in your output file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5osagFf5Fox"
      },
      "outputs": [],
      "source": [
        "model = # Model Architecture\n",
        "ckpt = torch.load(\"/content/best.pkl\")\n",
        "model.load_state_dict(ckpt[\"state_dict\"])\n",
        "\n",
        "test_dataset = ChestXRayDataset(\"\"\"Fill in args here\"\"\")\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=4, shuffle=False, drop_last=False)\n",
        "\n",
        "test_results = {\"image_path\": [], \"pred\": []}\n",
        "# Write method to load in data from test_loader, compute model predictions, and append results to test_results dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-6CgEWNWHdl"
      },
      "outputs": [],
      "source": [
        "test_results = pd.DataFrame(test_results)\n",
        "test_results.to_csv(f\"/content/test.csv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
