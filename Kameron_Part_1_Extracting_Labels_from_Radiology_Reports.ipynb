{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gqh0F-60baUC"
      },
      "source": [
        "## AIMI High School Internship 2023\n",
        "### Notebook 1: Extracting Labels from Radiology Reports\n",
        "\n",
        "**The Problem**: Given a chest X-ray, our goal in this project is to predict the distance from an endotracheal tube to the carina. This is an important clinical task - endotracheal tubes that are positioned too far (>5cm) above the carina will not work effectively.\n",
        "\n",
        "In order to train a model that can predict tube distances given chest X-rays, we require a ***training set*** with chest X-rays and labeled tube distances. However, when working with real-world medical data, important labels (e.g. endotracheal tube distances) are often not annotated ahead of time. The only data that a researcher has access to are the raw images and free-form clinical text written by the radiologist.\n",
        "\n",
        "**Your First Task**: Given a set of chest X-rays and paired radiology reports, your goal is to use natural language processing tools to extract endotracheal tube distances from the reports.\n",
        "\n",
        "**Looking Ahead**: When you complete this task, you should have a training dataset with chest X-rays labeled with endotracheal tube distances. You will later use this dataset to train a computer vision model that predicts the tube distance given an image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_75WVQT-eRDT"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9jmHkNcxt5w"
      },
      "source": [
        "Upload `data.zip`. It should take about 10 minutes for these files to be uploaded. Then, run the following cells to unzip the dataset (which should take < 10 seconds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03dEuEHVeUYs"
      },
      "source": [
        "### Understanding the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5765hi_9kSbE"
      },
      "source": [
        "Let's first go through some terminology. Medical data is often stored in a hierarchy consisting of three levels: patient, study, and images.\n",
        "- Patient: A patient is a single unique individual.\n",
        "- Study: Each patient may have multiple sets of images taken, perhaps on different days. Each set of images is referred to as a *study*.\n",
        "- Images: Each study consists of one or more *images*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVBKP0ZgpEa7"
      },
      "source": [
        "Chest X-ray images and radiology reports are stored in `data/` and are organized as follows:\n",
        "- `data/mimic-train`:\n",
        "  - Images: The MIMIC training set consists of 5313 subfolders, each representing a patient. Every patient has one or more studies, which are stored as subfolders. Images are stored in study folders as `.jpg` files with 512x512 pixels.\n",
        "  - Text: Reports are stored in patient folders with  `.txt` extensions. The filename corresponds to the study id and the content of the report applies to all images in the corresponding study.\n",
        "- `data/mimic-test`: The MIMIC test set is organized in a similar fashion as the MIMIC training set. Note that this is a held-out test set with 500 images that we will use for scoring models, so reports are not provided!\n",
        "- `data/mimic_train_student.csv`: This spreadsheet provides mappings between image paths, report paths, patient ids, study ids, and image ids for samples in the training set.\n",
        "- `data/mimic_test_student.csv`: This spreadsheet provides mappings between image paths, patient ids, study ids, and image ids for samples in the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "3NUgD5CAeXmU"
      },
      "outputs": [],
      "source": [
        "# Example Image\n",
        "from PIL import Image\n",
        "img = Image.open(f\"data/mimic-train/12000/59707/90529.jpg\")\n",
        "img.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "pM20ZAJYfrGI"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['                                 FINAL REPORT\\n',\n",
              " ' PORTABLE CHEST ___\\n',\n",
              " ' \\n',\n",
              " ' COMPARISON:  ___ radiograph.\\n',\n",
              " ' \\n',\n",
              " ' FINDINGS:  Tip of endotracheal tube terminates 6 cm above the carina. \\n',\n",
              " ' Cardiomediastinal contours are normal in appearance, and lungs are grossly\\n',\n",
              " ' clear.\\n']"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example Text Report\n",
        "with open(f\"data/mimic-train/12000/59707.txt\", \"r\") as f:\n",
        "  txt = f.readlines()\n",
        "txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Ot-DSq6nzlsl"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>split</th>\n",
              "      <th>patient_id</th>\n",
              "      <th>study_id</th>\n",
              "      <th>image_id</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>test</td>\n",
              "      <td>10345</td>\n",
              "      <td>50410</td>\n",
              "      <td>80276</td>\n",
              "      <td>mimic-test/10345/50410/80276.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>test</td>\n",
              "      <td>10345</td>\n",
              "      <td>50232</td>\n",
              "      <td>80350</td>\n",
              "      <td>mimic-test/10345/50232/80350.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>test</td>\n",
              "      <td>10189</td>\n",
              "      <td>50388</td>\n",
              "      <td>80353</td>\n",
              "      <td>mimic-test/10189/50388/80353.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>test</td>\n",
              "      <td>10127</td>\n",
              "      <td>50441</td>\n",
              "      <td>80124</td>\n",
              "      <td>mimic-test/10127/50441/80124.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>test</td>\n",
              "      <td>10004</td>\n",
              "      <td>50475</td>\n",
              "      <td>80218</td>\n",
              "      <td>mimic-test/10004/50475/80218.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>test</td>\n",
              "      <td>10252</td>\n",
              "      <td>50003</td>\n",
              "      <td>80193</td>\n",
              "      <td>mimic-test/10252/50003/80193.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>test</td>\n",
              "      <td>10363</td>\n",
              "      <td>50408</td>\n",
              "      <td>80072</td>\n",
              "      <td>mimic-test/10363/50408/80072.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>test</td>\n",
              "      <td>10363</td>\n",
              "      <td>50145</td>\n",
              "      <td>80039</td>\n",
              "      <td>mimic-test/10363/50145/80039.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>test</td>\n",
              "      <td>10054</td>\n",
              "      <td>50265</td>\n",
              "      <td>80374</td>\n",
              "      <td>mimic-test/10054/50265/80374.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>test</td>\n",
              "      <td>10058</td>\n",
              "      <td>50398</td>\n",
              "      <td>80032</td>\n",
              "      <td>mimic-test/10058/50398/80032.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    split  patient_id  study_id  image_id                        image_path\n",
              "0    test       10345     50410     80276  mimic-test/10345/50410/80276.jpg\n",
              "1    test       10345     50232     80350  mimic-test/10345/50232/80350.jpg\n",
              "2    test       10189     50388     80353  mimic-test/10189/50388/80353.jpg\n",
              "3    test       10127     50441     80124  mimic-test/10127/50441/80124.jpg\n",
              "4    test       10004     50475     80218  mimic-test/10004/50475/80218.jpg\n",
              "..    ...         ...       ...       ...                               ...\n",
              "495  test       10252     50003     80193  mimic-test/10252/50003/80193.jpg\n",
              "496  test       10363     50408     80072  mimic-test/10363/50408/80072.jpg\n",
              "497  test       10363     50145     80039  mimic-test/10363/50145/80039.jpg\n",
              "498  test       10054     50265     80374  mimic-test/10054/50265/80374.jpg\n",
              "499  test       10058     50398     80032  mimic-test/10058/50398/80032.jpg\n",
              "\n",
              "[500 rows x 5 columns]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load csv file with mappings\n",
        "import pandas as pd\n",
        "subjects = pd.read_csv(f'data/mimic_test_student.csv')\n",
        "\n",
        "subjects = subjects.drop(columns=[\"Unnamed: 0\"])\n",
        "subjects\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqmARLEuhaGP"
      },
      "source": [
        "### Extracting Tube Distance Labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwGvV1LZnfcq"
      },
      "source": [
        "You're now ready to begin this task! Keep in mind that not every chest X-ray provided in the training set contains endotracheal tube distance information, and there may be several edge cases to consider."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "TgeTSjDfheAX"
      },
      "outputs": [],
      "source": [
        "def is_cm(string):\n",
        "  valid = \"0123456789., \"\n",
        "  unit = None\n",
        "  for char in string:\n",
        "    if valid.find(char) == -1:\n",
        "      unit = char\n",
        "      break\n",
        "  return unit\n",
        "\n",
        "def validate(string):\n",
        "  valid = \"0123456789CcentiMmrl,. \"\n",
        "\n",
        "  for i, char in enumerate(string):\n",
        "    if valid.find(char) == -1:\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "def handle_no_space(string):\n",
        "  valid = \"0123456789\"\n",
        "  end_of_int = 0\n",
        "\n",
        "  for i, char in enumerate(string):\n",
        "    if valid.find(char) == -1:\n",
        "      end_of_int = i - 1\n",
        "  return string[:end_of_int] + \".0\"\n",
        "\n",
        "def handle_space(string):\n",
        "  space_loc = string.find(\" \")\n",
        "  if space_loc != -1:\n",
        "      return string.split(\" \")[0] + \".0\"\n",
        "  return handle_no_space(string)\n",
        "\n",
        "def handle_period(string):\n",
        "  out = \"\"\n",
        "  valid = \"0123456789.\"\n",
        "  for i, char in enumerate(string):\n",
        "    if valid.find(char) != -1:\n",
        "      out += char\n",
        "  if out[len(out) - 1] == \".\":\n",
        "    out = out[:len(out) - 1]\n",
        "    return out\n",
        "  return out\n",
        "\n",
        "def parse_measure(measurement):\n",
        "  comma_loc = measurement.find(\",\")\n",
        "  period_loc = measurement.find(\".\")\n",
        "  # no comma or period\n",
        "  if period_loc == -1 and comma_loc == -1:\n",
        "    return handle_space(measurement)\n",
        "  # only a comma\n",
        "  if comma_loc != -1:\n",
        "    measurement = measurement.replace(\",\", \".\")\n",
        "  # at this point, all strings that made it\n",
        "  # this far have only a period\n",
        "  return handle_period(measurement)\n",
        "\n",
        "def parse_report(path):\n",
        "  with open(f\"data/{path}\", \"r\") as f:\n",
        "    txt = f.readlines()\n",
        "  return \"\".join(txt).replace(\"\\n\", \"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5hpUKd6jsAa"
      },
      "source": [
        "## Measurement parsing test harness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "2VPXSSd7ei3B"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "# Test 1: 3.5 cm\n",
        "print(\"3.5\" == parse_measure(\"3.5 cm\"))\n",
        "# parse_measure(\"3.5 cm\")\n",
        "\n",
        "# Test 2: 3,5 cm\n",
        "print(\"3.5\" == parse_measure(\"3,5 cm\"))\n",
        "# parse_measure(\"3,5 cm\")\n",
        "\n",
        "# Test 3: 3 . 5 centimeter\n",
        "print(\"3.5\" == parse_measure(\"3 . 5 centimeter\"))\n",
        "# parse_measure(\"3 . 5 centimeter\")\n",
        "\n",
        "# Test 4: 3.5centimeters\n",
        "print(\"3.5\" == parse_measure(\"3.5centimeters\"))\n",
        "# parse_measure(\"3.5centimeters\")\n",
        "\n",
        "# Test 5: 3,5 centimetrs\n",
        "print(\"3.5\" == parse_measure(\"3,5 centimetrs\"))\n",
        "# parse_measure(\"3,5 centimetrs\")\n",
        "\n",
        "# Test 6: 3 cm\n",
        "print(\"3.0\" == parse_measure(\"3 cm\"))\n",
        "# parse_measure(\"3 cm\")\n",
        "\n",
        "# Test 7: 3.0 cm\n",
        "print(\"3.0\" == parse_measure(\"3.0 cm\"))\n",
        "# parse_measure(\"3.0 cm\")\n",
        "\n",
        "# Test 8: 12.5 cm\n",
        "print(\"12.5\" == parse_measure(\"12.5. cm\"))\n",
        "#print(parse_measure(\"12.5. cm\"))\n",
        "\n",
        "# Test 9: 22,5 cm\n",
        "print(\"22.5\" == parse_measure(\"22,5 cm\"))\n",
        "# parse_measure(\"3,5 cm\")\n",
        "\n",
        "# Test 10: 25 . 5 centimeter\n",
        "print(\"25.5\" == parse_measure(\"25 . 5 centimeter\"))\n",
        "# parse_measure(\"3 . 5 centimeter\")\n",
        "\n",
        "# Test 11: 361.5centimeters\n",
        "print(\"361.5\" == parse_measure(\"361.5centimeters\"))\n",
        "# parse_measure(\"3.5centimeters\")\n",
        "\n",
        "# Test 12: 46,5 centimetrs\n",
        "print(\"46.5\" == parse_measure(\"46,5 centimetrs\"))\n",
        "# parse_measure(\"3,5 centimetrs\")\n",
        "\n",
        "# Test 13: 75 cm\n",
        "print(\"75.0\" == parse_measure(\"75 cm\"))\n",
        "# parse_measure(\"3 cm\")\n",
        "\n",
        "# Test 14: 232.0 cm\n",
        "print(\"232.0\" == parse_measure(\"232.0 cm\"))\n",
        "# parse_measure(\"3.0 cm\")\n",
        "\n",
        "# Test 15: 232.0 cm\n",
        "print(\"232.0\" == parse_measure(\"2 3 2 .    0 cm\"))\n",
        "# parse_measure(\"3.0 cm\")\n",
        "\n",
        "# Test 16: 232 mm\n",
        "print(\"m\" == is_cm(\"232 mm\"))\n",
        "\n",
        "# Test 17: 232.0 millimietr\n",
        "print(\"m\" == is_cm(\"232.0 millimietr\"))\n",
        "\n",
        "# Test 18: 257 , 0 cm\n",
        "print(\"c\" == is_cm(\"257 , 0 cm\"))\n",
        "\n",
        "# Test 19: 25 , 0 cm\n",
        "print(\"c\" == is_cm(\"257 , 0 cm\"))\n",
        "\n",
        "# Test 20: 25\n",
        "print(None == is_cm(\"25\"))\n",
        "\n",
        "# Test 21: 2 and 3 cm\n",
        "print(False == validate(\"2 and 3 cm\"))\n",
        "\n",
        "# Test 22: 2 or 3 cm\n",
        "print(False == validate(\"2 or 3 cm\"))\n",
        "\n",
        "# Test 23: 2-3 cm\n",
        "print(False == validate(\"2-3 cm\"))\n",
        "\n",
        "# Test 24: less than 2 cm:\n",
        "print(False == validate(\"less than 2 cm\"))\n",
        "\n",
        "# Test 25: 2 away\n",
        "print(False == validate(\"2 away\"))\n",
        "\n",
        "# Test 26:\n",
        "print(True == validate(\"2 3 2 .    0 cm\"))\n",
        "\n",
        "# Test 27:\n",
        "print(True == validate(\"25 . 5 centimeter\"))\n",
        "\n",
        "# Test 28:\n",
        "print(True == validate(\"22,5 cm\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "tPFfGRZ3k3FC"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoModelForQuestionAnswering\n",
        "\n",
        " # pass device=0 if using gpu\n",
        "\n",
        "def biomed_token_class(report):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"d4data/biomedical-ner-all\")\n",
        "  model = AutoModelForTokenClassification.from_pretrained(\"d4data/biomedical-ner-all\")\n",
        "  pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
        "  return pipe(report)\n",
        "\n",
        "def roberta_squad2(report):\n",
        "  question = \"What is the exact distance between the ETT device and the carina?\"\n",
        "\n",
        "  model_name = \"deepset/roberta-base-squad2\"\n",
        "  model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  pipe = pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
        "\n",
        "  qa_input = {\n",
        "      'question': question,\n",
        "      'context': report\n",
        "  }\n",
        "\n",
        "  return pipe(qa_input)['answer']\n",
        "\n",
        "def get_distance(report):\n",
        "  measure_roberta = roberta_squad2(report)\n",
        "  roberta_valid = validate(measure_roberta)\n",
        "  if not roberta_valid:\n",
        "    return \"-1\"\n",
        "  if roberta_valid and is_cm(measure_roberta) is not None:\n",
        "    return measure_roberta\n",
        "  return \"-2\"\n",
        "\n",
        "def get_biomed_token_class(report):\n",
        "  response = biomed_token_class(report)\n",
        "  measure = \"-1\"\n",
        "  max_score = 0\n",
        "  for json in response:\n",
        "    if json[\"entity_group\"] == \"Distance\" and json[\"score\"] > max_score:\n",
        "      measure = json[\"word\"]\n",
        "      max_score = json[\"score\"]\n",
        "  return measure\n",
        "\n",
        "NOT_VALID = -1.0\n",
        "NO_UNITS = -2.0\n",
        "\n",
        "def process_volume(path):\n",
        "  report = parse_report(path)\n",
        "  distance = get_distance(report)\n",
        "  if distance == \"-1\":\n",
        "    return NOT_VALID\n",
        "  if distance == \"-2\":\n",
        "    return NO_UNITS\n",
        "  unit = is_cm(distance)\n",
        "  measurement = parse_measure(distance)\n",
        "  if unit == \"m\" or unit == \"M\":\n",
        "    return float(measurement) / 10.0\n",
        "  return float(measurement)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "d8j6QhxjgTqd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress checkpoint, processed 0 volumes. 12245 remain.\n",
            "Saving 0 data points to data/batch-0.csv\n",
            "Progress checkpoint, processed 100 volumes. 12145 remain.\n",
            "Progress checkpoint, processed 200 volumes. 12045 remain.\n",
            "Progress checkpoint, processed 300 volumes. 11945 remain.\n",
            "Progress checkpoint, processed 400 volumes. 11845 remain.\n",
            "Progress checkpoint, processed 500 volumes. 11745 remain.\n",
            "Progress checkpoint, processed 600 volumes. 11645 remain.\n",
            "Progress checkpoint, processed 700 volumes. 11545 remain.\n",
            "Progress checkpoint, processed 800 volumes. 11445 remain.\n",
            "Progress checkpoint, processed 900 volumes. 11345 remain.\n",
            "Progress checkpoint, processed 1000 volumes. 11245 remain.\n",
            "Saving 1000 data points to data/batch-1000.csv\n",
            "Progress checkpoint, processed 1100 volumes. 11145 remain.\n",
            "Progress checkpoint, processed 1200 volumes. 11045 remain.\n",
            "Progress checkpoint, processed 1300 volumes. 10945 remain.\n",
            "Progress checkpoint, processed 1400 volumes. 10845 remain.\n",
            "Progress checkpoint, processed 1500 volumes. 10745 remain.\n",
            "Progress checkpoint, processed 1600 volumes. 10645 remain.\n",
            "Progress checkpoint, processed 1700 volumes. 10545 remain.\n",
            "Progress checkpoint, processed 1800 volumes. 10445 remain.\n",
            "Progress checkpoint, processed 1900 volumes. 10345 remain.\n",
            "Progress checkpoint, processed 2000 volumes. 10245 remain.\n",
            "Saving 2000 data points to data/batch-2000.csv\n",
            "Progress checkpoint, processed 2100 volumes. 10145 remain.\n",
            "Progress checkpoint, processed 2200 volumes. 10045 remain.\n",
            "Progress checkpoint, processed 2300 volumes. 9945 remain.\n",
            "Progress checkpoint, processed 2400 volumes. 9845 remain.\n",
            "Progress checkpoint, processed 2500 volumes. 9745 remain.\n",
            "Progress checkpoint, processed 2600 volumes. 9645 remain.\n",
            "Progress checkpoint, processed 2700 volumes. 9545 remain.\n",
            "Progress checkpoint, processed 2800 volumes. 9445 remain.\n",
            "Progress checkpoint, processed 2900 volumes. 9345 remain.\n",
            "Progress checkpoint, processed 3000 volumes. 9245 remain.\n",
            "Saving 3000 data points to data/batch-3000.csv\n",
            "Progress checkpoint, processed 3100 volumes. 9145 remain.\n",
            "Progress checkpoint, processed 3200 volumes. 9045 remain.\n",
            "Progress checkpoint, processed 3300 volumes. 8945 remain.\n",
            "Progress checkpoint, processed 3400 volumes. 8845 remain.\n",
            "Progress checkpoint, processed 3500 volumes. 8745 remain.\n",
            "Progress checkpoint, processed 3600 volumes. 8645 remain.\n",
            "Progress checkpoint, processed 3700 volumes. 8545 remain.\n",
            "Progress checkpoint, processed 3800 volumes. 8445 remain.\n",
            "Progress checkpoint, processed 3900 volumes. 8345 remain.\n",
            "Progress checkpoint, processed 4000 volumes. 8245 remain.\n",
            "Saving 4000 data points to data/batch-4000.csv\n",
            "Progress checkpoint, processed 4100 volumes. 8145 remain.\n",
            "Progress checkpoint, processed 4200 volumes. 8045 remain.\n",
            "Progress checkpoint, processed 4300 volumes. 7945 remain.\n",
            "Progress checkpoint, processed 4400 volumes. 7845 remain.\n",
            "Progress checkpoint, processed 4500 volumes. 7745 remain.\n",
            "Progress checkpoint, processed 4600 volumes. 7645 remain.\n",
            "Progress checkpoint, processed 4700 volumes. 7545 remain.\n",
            "Progress checkpoint, processed 4800 volumes. 7445 remain.\n",
            "Progress checkpoint, processed 4900 volumes. 7345 remain.\n",
            "Progress checkpoint, processed 5000 volumes. 7245 remain.\n",
            "Saving 5000 data points to data/batch-5000.csv\n",
            "Progress checkpoint, processed 5100 volumes. 7145 remain.\n",
            "Progress checkpoint, processed 5200 volumes. 7045 remain.\n",
            "Progress checkpoint, processed 5300 volumes. 6945 remain.\n",
            "Progress checkpoint, processed 5400 volumes. 6845 remain.\n",
            "Progress checkpoint, processed 5500 volumes. 6745 remain.\n",
            "Progress checkpoint, processed 5600 volumes. 6645 remain.\n",
            "Progress checkpoint, processed 5700 volumes. 6545 remain.\n",
            "Progress checkpoint, processed 5800 volumes. 6445 remain.\n",
            "Progress checkpoint, processed 5900 volumes. 6345 remain.\n",
            "Progress checkpoint, processed 6000 volumes. 6245 remain.\n",
            "Saving 6000 data points to data/batch-6000.csv\n",
            "Progress checkpoint, processed 6100 volumes. 6145 remain.\n",
            "Progress checkpoint, processed 6200 volumes. 6045 remain.\n",
            "Progress checkpoint, processed 6300 volumes. 5945 remain.\n",
            "Progress checkpoint, processed 6400 volumes. 5845 remain.\n",
            "Progress checkpoint, processed 6500 volumes. 5745 remain.\n",
            "Progress checkpoint, processed 6600 volumes. 5645 remain.\n",
            "Progress checkpoint, processed 6700 volumes. 5545 remain.\n",
            "Progress checkpoint, processed 6800 volumes. 5445 remain.\n",
            "Progress checkpoint, processed 6900 volumes. 5345 remain.\n",
            "Progress checkpoint, processed 7000 volumes. 5245 remain.\n",
            "Saving 7000 data points to data/batch-7000.csv\n",
            "Progress checkpoint, processed 7100 volumes. 5145 remain.\n",
            "Progress checkpoint, processed 7200 volumes. 5045 remain.\n",
            "Progress checkpoint, processed 7300 volumes. 4945 remain.\n",
            "Progress checkpoint, processed 7400 volumes. 4845 remain.\n",
            "Progress checkpoint, processed 7500 volumes. 4745 remain.\n",
            "Progress checkpoint, processed 7600 volumes. 4645 remain.\n",
            "Progress checkpoint, processed 7700 volumes. 4545 remain.\n",
            "Progress checkpoint, processed 7800 volumes. 4445 remain.\n",
            "Progress checkpoint, processed 7900 volumes. 4345 remain.\n",
            "Progress checkpoint, processed 8000 volumes. 4245 remain.\n",
            "Saving 8000 data points to data/batch-8000.csv\n",
            "Progress checkpoint, processed 8100 volumes. 4145 remain.\n",
            "Progress checkpoint, processed 8200 volumes. 4045 remain.\n",
            "Progress checkpoint, processed 8300 volumes. 3945 remain.\n",
            "Progress checkpoint, processed 8400 volumes. 3845 remain.\n",
            "Progress checkpoint, processed 8500 volumes. 3745 remain.\n",
            "Progress checkpoint, processed 8600 volumes. 3645 remain.\n",
            "Progress checkpoint, processed 8700 volumes. 3545 remain.\n",
            "Progress checkpoint, processed 8800 volumes. 3445 remain.\n",
            "Progress checkpoint, processed 8900 volumes. 3345 remain.\n",
            "Progress checkpoint, processed 9000 volumes. 3245 remain.\n",
            "Saving 9000 data points to data/batch-9000.csv\n",
            "Progress checkpoint, processed 9100 volumes. 3145 remain.\n",
            "Progress checkpoint, processed 9200 volumes. 3045 remain.\n",
            "Progress checkpoint, processed 9300 volumes. 2945 remain.\n",
            "Progress checkpoint, processed 9400 volumes. 2845 remain.\n",
            "Progress checkpoint, processed 9500 volumes. 2745 remain.\n",
            "Progress checkpoint, processed 9600 volumes. 2645 remain.\n",
            "Progress checkpoint, processed 9700 volumes. 2545 remain.\n",
            "Progress checkpoint, processed 9800 volumes. 2445 remain.\n",
            "Progress checkpoint, processed 9900 volumes. 2345 remain.\n",
            "Progress checkpoint, processed 10000 volumes. 2245 remain.\n",
            "Saving 10000 data points to data/batch-10000.csv\n",
            "Progress checkpoint, processed 10100 volumes. 2145 remain.\n",
            "Progress checkpoint, processed 10200 volumes. 2045 remain.\n",
            "Progress checkpoint, processed 10300 volumes. 1945 remain.\n",
            "Progress checkpoint, processed 10400 volumes. 1845 remain.\n",
            "Progress checkpoint, processed 10500 volumes. 1745 remain.\n",
            "Progress checkpoint, processed 10600 volumes. 1645 remain.\n",
            "Progress checkpoint, processed 10700 volumes. 1545 remain.\n",
            "Progress checkpoint, processed 10800 volumes. 1445 remain.\n",
            "Progress checkpoint, processed 10900 volumes. 1345 remain.\n",
            "Progress checkpoint, processed 11000 volumes. 1245 remain.\n",
            "Saving 11000 data points to data/batch-11000.csv\n",
            "Progress checkpoint, processed 11100 volumes. 1145 remain.\n",
            "Progress checkpoint, processed 11200 volumes. 1045 remain.\n",
            "Progress checkpoint, processed 11300 volumes. 945 remain.\n",
            "Progress checkpoint, processed 11400 volumes. 845 remain.\n",
            "Progress checkpoint, processed 11500 volumes. 745 remain.\n",
            "Progress checkpoint, processed 11600 volumes. 645 remain.\n",
            "Progress checkpoint, processed 11700 volumes. 545 remain.\n",
            "Progress checkpoint, processed 11800 volumes. 445 remain.\n",
            "Progress checkpoint, processed 11900 volumes. 345 remain.\n",
            "Progress checkpoint, processed 12000 volumes. 245 remain.\n",
            "Saving 12000 data points to data/batch-12000.csv\n",
            "Progress checkpoint, processed 12100 volumes. 145 remain.\n",
            "Progress checkpoint, processed 12200 volumes. 45 remain.\n",
            "Task completed.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def good_positioning(measure):\n",
        "  if measure < 0:\n",
        "    return -1\n",
        "  if measure <= 5:\n",
        "    return 1\n",
        "  return 0\n",
        "\n",
        "\n",
        "# Load csv file with mappings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def get_labels(data):\n",
        "  PATH = f\"data/mimic_{data}_student.csv\"\n",
        "  subjects = pd.read_csv(PATH)\n",
        "  subjects = subjects.drop(columns=[\"Unnamed: 0\", \"study_id\", \"image_id\"])\n",
        "  report_paths = subjects[\"report_path\"].to_numpy()\n",
        "  measures = []\n",
        "  positioning = [] # 1 is good, 0 is bad\n",
        "\n",
        "  for i, path in enumerate(report_paths):\n",
        "    if i % 100 == 0:\n",
        "      print(f\"Progress checkpoint, processed {i} volumes. {len(report_paths) - (i)} remain.\")\n",
        "    if i % 1000 == 0:\n",
        "      print(f\"Saving {i} data points to data/batch-{i}.csv\")\n",
        "      checkpoint = pd.DataFrame()\n",
        "      checkpoint[\"measures\"] = measures\n",
        "      checkpoint[\"positioning\"] = positioning\n",
        "      checkpoint.to_csv(f\"data/batch-{i}.csv\")\n",
        "\n",
        "    measure = process_volume(path)\n",
        "    measures.append(measure)\n",
        "    positioning.append(good_positioning(measure))\n",
        "\n",
        "  subjects[\"measures\"] = measures\n",
        "  subjects[\"positioning\"] = positioning\n",
        "  subjects.to_csv(f\"data/mimic_{data}_labels.csv\")\n",
        "\n",
        "get_labels(data=\"train\")\n",
        "print(\"Task completed.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
